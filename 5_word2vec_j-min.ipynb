{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the data from the source website if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read the data into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
      "Sample count [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n",
      "Sample dict [('fawn', 45848), ('homomorphism', 9648), ('nordisk', 39343), ('nunnery', 36075), ('chthonic', 33554), ('sowell', 40562), ('sonja', 38175), ('showa', 32906), ('woods', 6263), ('hsv', 44222)]\n",
      "Sample Reverse Dictionary [(0, 'UNK'), (1, 'the'), (2, 'of'), (3, 'and'), (4, 'one'), (5, 'in'), (6, 'a'), (7, 'to'), (8, 'zero'), (9, 'nine')]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    dictionary = dict()\n",
    "        \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) # dictionary indexing\n",
    "        #print (dictionary)\n",
    "    \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    # dictionary에 있으면 해당 index, 아니면(UNK) index =0\n",
    "    # unk 의 전체 개수 세기\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "            # 위에서 len으로 정의한 dictionary의 단어 indexing\n",
    "            # index가 낮을수록 빈번\n",
    "            \n",
    "        else:\n",
    "            index = 0 # dictionary['UNK']\n",
    "            unk_count = unk_count +1\n",
    "        data.append(index) # 각 단어들의 index를 data 에 입력\n",
    "        \n",
    "    count[0][1] = unk_count # UNK의 개수\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    # reverse_dictionary: dictionary의 key와 value를 변경\n",
    "    # index(key)가 낮을수록 빈번\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "print('Sample count', count[:10])\n",
    "print('Sample dict', dictionary.items()[:10])\n",
    "print('Sample Reverse Dictionary', reverse_dictionary.items()[:10])\n",
    "del words # Reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(count))\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    input\n",
    "    \n",
    "    batch_size:    batch size\n",
    "    num_skips:     context window 내에서의 예측값 후보 개수\n",
    "    skip_window:   context window 크기\n",
    "    \n",
    "    \n",
    "    return\n",
    "    \n",
    "    batch:         mini-batch of data\n",
    "    labels:        labels of mini-batch\n",
    "    \n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0 # batch_size는 num_skips의 배수\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape =(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape =(batch_size, 1), dtype = np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip window         target        skip window ]\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    # deque: double ended queue, collecions 모듈의 확장된 list 클래스\n",
    "    # 양쪽에 모두 pop, push 할 수 있음\n",
    "    # append를 하는데 maxlen 을 넘어가면 왼쪽으로 한칸씩 밀림\n",
    "    \n",
    "    \n",
    "    # 매 batch마다 buffer에 data 입력\n",
    "    # buffer = data[data_index : data_index + span]\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    #print(buffer)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            \n",
    "            # target 에 skip_window + 1 ~ 2 x skip_window 중 하나의 정수를\n",
    "            # targets_to_avoid를 이용해 랜덤하게 입력\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0,span-1)\n",
    "            targets_to_avoid.append(target) # targets_to_avoid = [skip_window, target-1, ..]\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "            \n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        #print(buffer)\n",
    "    \n",
    "    #print(buffer)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating batch ... \n",
      "Sample batches:  [ 12  12   6   6 195 195   2   2]\n",
      "Sample labels:  [[5239]\n",
      " [3084]\n",
      " [   2]\n",
      " [3084]\n",
      " [  12]\n",
      " [   2]\n",
      " [ 195]\n",
      " [  46]]\n",
      "12 -> 5239\n",
      "as -> anarchism\n",
      "12 -> 3084\n",
      "as -> originated\n",
      "6 -> 2\n",
      "a -> of\n",
      "6 -> 3084\n",
      "a -> originated\n",
      "195 -> 12\n",
      "term -> as\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating batch ... \")\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)\n",
    "print(\"Sample batches: \", batch[:])\n",
    "print(\"Sample labels: \", labels[:])\n",
    "for i in range(5):\n",
    "    print(batch[i], '->', labels[i,0])\n",
    "    print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12  12   6   6 195 195   2   2]\n",
      "[[5239]\n",
      " [3084]\n",
      " [   2]\n",
      " [3084]\n",
      " [  12]\n",
      " [   2]\n",
      " [ 195]\n",
      " [  46]]\n"
     ]
    }
   ],
   "source": [
    "print(batch)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1\n",
      "    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']\n",
      "    labels: ['as', 'anarchism', 'a', 'originated', 'term', 'as', 'a', 'of']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2\n",
      "    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']\n",
      "    labels: ['originated', 'anarchism', 'a', 'term', 'originated', 'of', 'as', 'term']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data [:batch_size]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    \n",
    "    batch, labels = generate_batch(batch_size = batch_size, num_skips = num_skips,\n",
    "                                   skip_window = skip_window)\n",
    "    print(\"\\nwith num_skips = %d and skip_window = %d\" % (num_skips, skip_window))\n",
    "    print(\"    batch:\", [reverse_dictionary[bi] for bi in batch])\n",
    "    print(\"    labels:\", [reverse_dictionary[li] for li in labels.reshape(batch_size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a skip gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "\n",
    "valid_size  = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "# [0 ~ valid_wind]의 리스트에서 valid_size개 만큼 샘플링\n",
    "# 여기서는 0~99 중 랜덤하게 16개 샘플링\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():#, tf.device('/cpu:0'):\n",
    "    \n",
    "    # input data\n",
    "    train_dataset = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype = tf.int32)\n",
    "    \n",
    "    \n",
    "    # Variables\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    ) # 전체 vocabulary를 embedding_size 크기의 벡터로 embedding, -1.0 ~ 1.0 의 값으로 초기화\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                           stddev = 1.0 / math.sqrt(embedding_size))\n",
    "    )\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Model\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # 위에서 정의한 embeddings 중 train_dataset에 해당되는 단어들만 추출 => input\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                   train_labels, num_sampled, vocabulary_size\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    # Similarity\n",
    "    # similarity between minibatch ~ all embeddings\n",
    "    # cosine distance\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims = True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset\n",
    "    )\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, tf.transpose(normalized_embeddings)\n",
    "    ) # valid_dataset에 대응하는 cosine distance = valid_embeddings 와 normalized_embeddings 의 곱\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Average loss at step 0: 8.309031\n",
      "during\n",
      "Nearest to during: hoshea,  please,  merchants,  trent,  assented,  singularities,  everyday,  saloons, \n",
      "it\n",
      "Nearest to it: troubadour,  titus,  blot,  trois,  implications,  processor,  don,  ecb, \n",
      "called\n",
      "Nearest to called: beauvoir,  beacon,  beac,  retrieval,  lite,  jellyfish,  danubian,  mysticism, \n",
      "often\n",
      "Nearest to often: cantilever,  lavon,  jingles,  yorktown,  steele,  terminate,  late,  fabrication, \n",
      "so\n",
      "Nearest to so: manfred,  current,  beretta,  bookstore,  reestablish,  lagrangian,  harris,  pasted, \n",
      "three\n",
      "Nearest to three: signatures,  mishandling,  likud,  mauritian,  vermont,  oz,  nielsen,  french, \n",
      "also\n",
      "Nearest to also: trial,  instrumentation,  weaving,  lonely,  disguising,  episcopacy,  nobles,  lampoon, \n",
      "six\n",
      "Nearest to six: fixing,  xliii,  sato,  firmness,  vowed,  lob,  diphthong,  hang, \n",
      "on\n",
      "Nearest to on: ames,  gnosis,  lech,  hampered,  notional,  subjectivity,  kom,  stead, \n",
      "th\n",
      "Nearest to th: simeon,  gayoom,  utilization,  muskegon,  dynasty,  outlaws,  michell,  rabbi, \n",
      "eight\n",
      "Nearest to eight: pushes,  confiscated,  hi,  strips,  doctoral,  elsa,  crane,  buddhism, \n",
      "that\n",
      "Nearest to that: schoolgirl,  diminishes,  barons,  chapman,  supermodel,  hogarth,  acronym,  amal, \n",
      "has\n",
      "Nearest to has: ksh,  nasional,  malnutrition,  front,  spurring,  homeowners,  matthias,  acquainted, \n",
      "no\n",
      "Nearest to no: murakami,  fowler,  marangella,  flawed,  neighbour,  ensured,  torsional,  cosmopolitan, \n",
      "use\n",
      "Nearest to use: amortized,  eggs,  alpacas,  merodach,  troubleshooting,  obstructing,  waley,  procellariiformes, \n",
      "into\n",
      "Nearest to into: infiltrate,  kidnapped,  aha,  friendly,  krone,  patrilineal,  cirencester,  ldl, \n",
      "Average loss at step 2000: 4.360922\n",
      "Average loss at step 4000: 3.872114\n",
      "Average loss at step 6000: 3.793011\n",
      "Average loss at step 8000: 3.683390\n",
      "Average loss at step 10000: 3.611950\n",
      "Average loss at step 12000: 3.606958\n",
      "Average loss at step 14000: 3.571570\n",
      "Average loss at step 16000: 3.409382\n",
      "Average loss at step 18000: 3.455127\n",
      "Average loss at step 20000: 3.536718\n",
      "Average loss at step 22000: 3.503623\n",
      "Average loss at step 24000: 3.489517\n",
      "Average loss at step 26000: 3.478659\n",
      "Average loss at step 28000: 3.485018\n",
      "Average loss at step 30000: 3.503174\n",
      "during\n",
      "Nearest to during: in,  at,  sgt,  after,  renounced,  before,  please,  nursery, \n",
      "it\n",
      "Nearest to it: he,  she,  there,  this,  they,  which,  ananda,  umts, \n",
      "called\n",
      "Nearest to called: used,  considered,  caliphate,  awacs,  swanson,  assertive,  interlude,  libertarian, \n",
      "often\n",
      "Nearest to often: usually,  also,  commonly,  still,  sometimes,  generally,  who,  there, \n",
      "so\n",
      "Nearest to so: neurotransmitter,  synaptic,  discouraged,  wandered,  utopia,  salty,  slightly,  sz, \n",
      "three\n",
      "Nearest to three: four,  five,  seven,  eight,  six,  two,  nine,  zero, \n",
      "also\n",
      "Nearest to also: often,  which,  now,  quite,  never,  it,  generally,  who, \n",
      "six\n",
      "Nearest to six: four,  eight,  nine,  five,  seven,  three,  two,  zero, \n",
      "on\n",
      "Nearest to on: in,  stead,  upon,  from,  at,  danubian,  through,  despite, \n",
      "th\n",
      "Nearest to th: muskegon,  bc,  six,  earns,  mottled,  nd,  ad,  lavrenty, \n",
      "eight\n",
      "Nearest to eight: nine,  four,  seven,  five,  six,  three,  zero,  two, \n",
      "that\n",
      "Nearest to that: which,  however,  but,  this,  pulaski,  milliseconds,  what,  moli, \n",
      "has\n",
      "Nearest to has: had,  have,  is,  was,  handler,  copylefted,  carthaginians,  disordered, \n",
      "no\n",
      "Nearest to no: seo,  peripatetic,  typically,  any,  a,  dispensation,  still,  allowable, \n",
      "use\n",
      "Nearest to use: merodach,  construction,  tretyakov,  rito,  average,  troubleshooting,  amortized,  indie, \n",
      "into\n",
      "Nearest to into: from,  through,  ldl,  terri,  billy,  jaw,  patrilineal,  panasonic, \n",
      "Average loss at step 32000: 3.499606\n",
      "Average loss at step 34000: 3.497464\n",
      "Average loss at step 36000: 3.454733\n",
      "Average loss at step 38000: 3.300024\n",
      "Average loss at step 40000: 3.428887\n",
      "Average loss at step 42000: 3.435647\n",
      "Average loss at step 44000: 3.456644\n",
      "Average loss at step 46000: 3.453365\n",
      "Average loss at step 48000: 3.350534\n",
      "Average loss at step 50000: 3.383759\n",
      "Average loss at step 52000: 3.438230\n",
      "Average loss at step 54000: 3.427834\n",
      "Average loss at step 56000: 3.439981\n",
      "Average loss at step 58000: 3.398168\n",
      "Average loss at step 60000: 3.395907\n",
      "during\n",
      "Nearest to during: after,  when,  before,  throughout,  since,  in,  at,  until, \n",
      "it\n",
      "Nearest to it: she,  he,  this,  there,  what,  they,  reluctant,  which, \n",
      "called\n",
      "Nearest to called: used,  considered,  named,  known,  jes,  qualified,  rockabilly,  subtle, \n",
      "often\n",
      "Nearest to often: usually,  sometimes,  commonly,  generally,  still,  also,  widely,  currently, \n",
      "so\n",
      "Nearest to so: manicheanism,  neurotransmitter,  too,  grievance,  if,  severn,  wreath,  sz, \n",
      "three\n",
      "Nearest to three: five,  four,  six,  eight,  two,  seven,  nine,  zero, \n",
      "also\n",
      "Nearest to also: now,  often,  still,  usually,  only,  commonly,  collectively,  sometimes, \n",
      "six\n",
      "Nearest to six: eight,  five,  four,  nine,  three,  seven,  two,  one, \n",
      "on\n",
      "Nearest to on: upon,  stead,  in,  through,  via,  knoll,  about,  within, \n",
      "th\n",
      "Nearest to th: bc,  ad,  five,  nineteenth,  nd,  six,  bce,  one, \n",
      "eight\n",
      "Nearest to eight: nine,  six,  seven,  four,  five,  three,  one,  two, \n",
      "that\n",
      "Nearest to that: which,  this,  however,  what,  marlins,  who,  usually,  where, \n",
      "has\n",
      "Nearest to has: had,  have,  was,  is,  although,  having,  pound,  copylefted, \n",
      "no\n",
      "Nearest to no: any,  seo,  wines,  peripatetic,  little,  stereo,  typically,  locality, \n",
      "use\n",
      "Nearest to use: merodach,  tretyakov,  tabled,  usage,  course,  cinematic,  ability,  form, \n",
      "into\n",
      "Nearest to into: through,  from,  rosario,  around,  billy,  over,  ldl,  down, \n",
      "Average loss at step 62000: 3.238418\n",
      "Average loss at step 64000: 3.254386\n",
      "Average loss at step 66000: 3.403016\n",
      "Average loss at step 68000: 3.393275\n",
      "Average loss at step 70000: 3.354037\n",
      "Average loss at step 72000: 3.378473\n",
      "Average loss at step 74000: 3.347153\n",
      "Average loss at step 76000: 3.316377\n",
      "Average loss at step 78000: 3.349241\n",
      "Average loss at step 80000: 3.381138\n",
      "Average loss at step 82000: 3.409560\n",
      "Average loss at step 84000: 3.406970\n",
      "Average loss at step 86000: 3.388825\n",
      "Average loss at step 88000: 3.350719\n",
      "Average loss at step 90000: 3.365615\n",
      "during\n",
      "Nearest to during: after,  in,  before,  until,  throughout,  while,  when,  from, \n",
      "it\n",
      "Nearest to it: he,  she,  there,  this,  they,  itself,  still,  soon, \n",
      "called\n",
      "Nearest to called: used,  named,  considered,  known,  referred,  kaleidoscope,  lighter,  jes, \n",
      "often\n",
      "Nearest to often: usually,  sometimes,  commonly,  generally,  still,  frequently,  also,  now, \n",
      "so\n",
      "Nearest to so: thus,  too,  if,  stated,  fact,  grievance,  focused,  salty, \n",
      "three\n",
      "Nearest to three: two,  four,  five,  seven,  eight,  six,  nine,  zero, \n",
      "also\n",
      "Nearest to also: never,  now,  often,  still,  which,  usually,  sometimes,  who, \n",
      "six\n",
      "Nearest to six: eight,  seven,  five,  four,  nine,  three,  two,  zero, \n",
      "on\n",
      "Nearest to on: upon,  under,  stead,  through,  via,  sunga,  pick,  in, \n",
      "th\n",
      "Nearest to th: bc,  nineteenth,  seven,  ad,  bce,  eight,  nd,  edgard, \n",
      "eight\n",
      "Nearest to eight: seven,  nine,  six,  five,  four,  three,  zero,  two, \n",
      "that\n",
      "Nearest to that: which,  however,  what,  marlins,  where,  but,  ferns,  who, \n",
      "has\n",
      "Nearest to has: had,  have,  was,  is,  having,  since,  includes,  requires, \n",
      "no\n",
      "Nearest to no: any,  little,  only,  another,  considerably,  neither,  traditionalists,  locality, \n",
      "use\n",
      "Nearest to use: course,  rito,  tretyakov,  tabled,  usage,  merodach,  resembled,  naghten, \n",
      "into\n",
      "Nearest to into: through,  from,  down,  within,  out,  back,  with,  chiles, \n",
      "Average loss at step 92000: 3.396037\n",
      "Average loss at step 94000: 3.251713\n",
      "Average loss at step 96000: 3.356120\n",
      "Average loss at step 98000: 3.234200\n",
      "Average loss at step 100000: 3.358105\n",
      "906.527242\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "tic = time.clock()\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('initialized')\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window\n",
    "        )\n",
    "        \n",
    "        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
    "        _, batch_loss = session.run([optimizer, loss], feed_dict = feed_dict)\n",
    "        \n",
    "        average_loss += batch_loss\n",
    "        \n",
    "        # 매 2000 스텝마다 average loss 출력\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 30000 == 0:\n",
    "            sim = similarity.eval()     \n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                print(valid_word)\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                \n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s, ' % (log, close_word)\n",
    "                print(log)\n",
    "                \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "toc = time.clock()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-371d58f74eb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.sum(final_embeddings[0]))\n",
    "print(np.sum(np.square(final_embeddings[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77c5307dae24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pca'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtwo_d_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity = 30, n_components = 2, init = 'pca', n_iter = 5000)\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def = plot(embeddings, lables):\n",
    "    assert embeddings.shape[0] >= len(labels)\n",
    "    pylab.figure(figsize = (15,15))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings = [i,:]\n",
    "        pylab.scatter(x,y)\n",
    "        pylab.annptate(label, xy = (x,y), xytext = (5,2), textcoords = 'offeset points',\n",
    "                       ha = 'right', va ='bottom')\n",
    "    pylab = show()\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(1. num_points + 1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
